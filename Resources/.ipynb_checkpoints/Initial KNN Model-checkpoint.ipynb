{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Nearest Neighbor search is used to find objects that are similar to each other. The idea is that given an input, NN search finds the objects in our database that are similar to the input. As a simple example, if you had a database of news articles and you want to retrieve news similar to your query then you would perform a nearest neighbors search for you input query against all articles in your database and return top 10 results.\n",
    "\n",
    "In NN search, distance function is important. It decides how similar or dissimilar the objects are. Lower distance values indicate the objects are similar where as higher indicate they are dissimilar. For example, if two objects have distance 0 then they are identical.\n",
    "\n",
    "## Data \n",
    "To keep things simple, we’ll use a dataset provided by scikit-learn library called 20 newsgroups. The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics. The code below will load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "bunch = fetch_20newsgroups(remove='headers')\n",
    "\n",
    "print(type(bunch), bunch.keys())\n",
    "# (sklearn.utils.Bunch, dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a basically a dict like object with the keys shown above. For this demo we are only interested in the values under data key. which contains a list of posts. As an example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bunch.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "As we know, we need to represent our entities in a vector space in order to use nearest neighbors. Since our entities are texts, we should use some feature extraction technique to extract feature vectors. For this demo, we’ll use Tf-Idf. \n",
    "\n",
    "To keep things simple, we won’t tune any parameters and use defaults except for max_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(max_features=10_000)\n",
    "features = vec.fit_transform(bunch.data)\n",
    "print(features.shape) # (11314, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have feature vectors for entire dataset where each vector has 10,000 elements. We can finally train a nearest neighbors model.\n",
    "\n",
    "## Model training\n",
    "This step is also very simple. You just need to instantiate a NearestNeighbors object and train it by calling the fit function.\n",
    "\n",
    "### Important Parameters\n",
    "Here are some important parameters that you might need to change depending on your needs.\n",
    "\n",
    "**n_neighbors** : Number of neighbors to use by default for kneighbors queries.\n",
    "\n",
    "**algorithm** : Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "‘ball_tree’ will use :class:BallTree\n",
    "‘kd_tree’ will use :class:KDTree\n",
    "‘brute’ will use a brute-force search.\n",
    "‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
    "metric : metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used. If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy’s metrics, but is less efficient than passing the metric name as a string\n",
    "\n",
    "**metric** : metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used. If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy’s metrics, but is less efficient than passing the metric name as a string.\n",
    "\n",
    "Valid values for metric are:\n",
    "\n",
    "[‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]\n",
    "any function from scipy.spatial.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "knn = NearestNeighbors(n_neighbors=10, metric='cosine')\n",
    "knn.fit(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all it takes to train a KNN model. I’ve used cosine as metric because it is generally used for text similarity. euclidean is also a good choice.\n",
    "\n",
    "## Results\n",
    "Now that we have a KNN model, how to find similar items for a given input text? We first need to convert the text into feature vector using vec.transform function and then give this vector as input to the knn model.\n",
    "\n",
    "Let’s quickly look at what output we get from knn. To find nearest neighbors, we need to call kneighbors function. The first parameter is a list of feature vectors. If return_distance is False, it only returns a 2D array where each row contains k nearest neighbors indices for each input feature vector.\n",
    "\n",
    "If return_distance is True, it returns a tuple of 2D arrays. In first array each row contains the distances and in the second array each row contains k nearest neighbors indices for each input feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.kneighbors(features[0:1], return_distance=False)\n",
    "# array([[   0,  958, 8013, 8266,  659, 5553, 3819, 2554, 6055, 7993]])\n",
    "\n",
    "knn.kneighbors(features[0:1], return_distance=True)\n",
    "# (array([[0.        , 0.35119023, 0.62822688, 0.64738668, 0.66613124,\n",
    "#         0.67267273, 0.68149664, 0.68833514, 0.70024449, 0.70169709]]),\n",
    "# array([[   0,  958, 8013, 8266,  659, 5553, 3819, 2554, 6055, 7993]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the indices returned are the index of feature vectors use while training the model knn.fit(features). Also, almost all functions like fit, transform, kneighbors etc. in sklearn expects 2D array as input so we passed features[0:1] as input rather than just features[0].\n",
    "\n",
    "Finally, the code below shows how you can take raw input texts, extract the features and find the nearest neighbors. Note here we have specifically specified n_neighbors=2 so that we get 2 neighbors per query. If this value is not specified then it will return 10 neighbors because we specified n_neighbors=10 while instantiating our model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\"any recommendations for good ftp sites?\", \"i need to clean my car\"]\n",
    "input_features = vec.transform(input_texts)\n",
    "\n",
    "D, N = knn.kneighbors(input_features, n_neighbors=2, return_distance=True)\n",
    "\n",
    "for input_text, distances, neighbors in zip(input_texts, D, N):\n",
    "    print(\"Input text = \", input_text[:200], \"\\n\")\n",
    "    for dist, neighbor_idx in zip(distances, neighbors):\n",
    "        print(\"Distance = \", dist, \"Neighbor idx = \", neighbor_idx)\n",
    "        print(bunch.data[neighbor_idx][:200])\n",
    "        print(\"-\"*200)\n",
    "    print(\"=\"*200)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
